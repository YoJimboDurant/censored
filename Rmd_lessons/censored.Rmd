
>These things cannot be explained in detail. From one thing, know ten thousand things. You must study hard.

- Miyamoto Musashi, _Go Rin No Sho_



# Introduction - why does this matter?

## Example Data - Arsenic in Apple Juice
```{r htmltable, echo=FALSE, message=FALSE, warning=FALSE}
require(htmltab)
require(tidyverse)
require(magrittr)
require(NADA)
require(gtable)

qq.line <- function(data, qf, na.rm) {
    # from stackoverflow.com/a/4357932/1346276
    q.sample <- quantile(data, c(0.25, 0.75), na.rm = na.rm)
    q.theory <- qf(c(0.25, 0.75))
    slope <- diff(q.sample) / diff(q.theory)
    intercept <- q.sample[1] - slope * q.theory[1]

    list(slope = slope, intercept = intercept)
}

StatQQLine <- ggproto("StatQQLine", Stat,
    # http://docs.ggplot2.org/current/vignettes/extending-ggplot2.html
    # https://github.com/hadley/ggplot2/blob/master/R/stat-qq.r

    required_aes = c('sample'),

    compute_group = function(data, scales,
                             distribution = stats::qnorm,
                             dparams = list(),
                             na.rm = FALSE) {
        qf <- function(p) do.call(distribution, c(list(p = p), dparams))

        n <- length(data$sample)
        theoretical <- qf(stats::ppoints(n))
        qq <- qq.line(data$sample, qf = qf, na.rm = na.rm)
        line <- qq$intercept + theoretical * qq$slope

        data.frame(x = theoretical, y = line)
    } 
)

stat_qqline <- function(mapping = NULL, data = NULL, geom = "line",
                        position = "identity", ...,
                        distribution = stats::qnorm,
                        dparams = list(),
                        na.rm = FALSE,
                        show.legend = NA, 
                        inherit.aes = TRUE) {
    layer(stat = StatQQLine, data = data, mapping = mapping, geom = geom,
          position = position, show.legend = show.legend, inherit.aes = inherit.aes,
          params = list(distribution = distribution,
                        dparams = dparams,
                        na.rm = na.rm, ...))
}


url <- "https://www.fda.gov/Food/FoodborneIllnessContaminants/Metals/ucm283725.htm"
juice <- htmltab(doc = url)
x <- as.numeric(as.character(juice[,3]))
x[x==0] <- 0.25
cen <- is.na(x)
xhalf <- x
xsqrt <- x
xzero <- x
x[cen] <- 2
xhalf[cen] <- 1
xsqrt[cen] <- 2/sqrt(2)
cen[x==0.25] <- TRUE
xhalf[x==0.25] <- 0.0
xsqrt[x==0.25] <- 0.0
xzero[cen] <- 0

dfx <- data.frame(x=x, xhalf = xhalf, xsqrt = xsqrt, xzero = xzero, cen=cen)
# 
# myx <- function(x) x
# 
# ros_apples <- ros(dfx$x, dfx$cen, forwardT="myx", reverseT="myx")

# ros_df <- as.data.frame(ros_apples)

myapples <- gather(dfx, key = "key", value = "obs", x, xhalf, xsqrt, xzero) %>%
  mutate( key = recode(key, x = "1 %*% DL", xhalf = "DL/2", xsqrt = "DL/sqrt(2)", xzero = "0%*%DL")) 

sumapples <- myapples %>%
  group_by(key) %>%
  summarise(average = signif(mean(obs),3),  `standard deviation` = signif(sd(obs),3)) 

saveRDS(myapples, file= "./apples.rds")

ggplot(myapples, aes(x=obs, fill = !cen)) +geom_histogram() + 
  facet_wrap(~key, labeller = label_parsed) + 
  scale_fill_manual("Detected", values = c("Grey", "Black")) +
  geom_vline(data=sumapples, aes( xintercept = average, color = "red", linetype = "dashed"), show.legend = FALSE) +
  xlab("Inorganic Arsenic Concentration [ppb]") + geom_text(data = sumapples, x = 6, y =18, aes(label = paste("bar(x)==", average)), parse = TRUE, inherit.aes = FALSE) +
  geom_text(data = sumapples, x = 6, y =12, aes(label = paste("s ==", `standard deviation`)), parse = TRUE, inherit.aes = FALSE) + 
  labs(title = "Inorganic Arsenic Concentration in Apple Juice", 
       subtitle = "2011 FDA Survey [n=94]", 
       caption = "Source:FDA. Results of Arsenic Analysis in Single-Strength Apple Juice, 2011\n (ORA Sampling Assignment 2011102701).") +
  theme_bw() +
  theme(axis.text=element_text(size=10),
        axis.title=element_text(size=10,face="bold"),
        plot.caption=element_text(size=8))

```

 
<article>
About 28% of these data are below detection limit
 - Mean varies by up to 0.5 ppb
- Standard Deviation varies by up to 0.66 ppb

What this means is there is variability in our answer could be based on how we selected our method of imputing non-detected data.
 

Which method is "correct"? 
What do we do if our results are sensitive to how we treated nondetect data?
Should any of our estimates of mean and standard deviation be dependent on the rate of nondetected data?


<\article>

## Simulation of Substitution Methods
```{r, echo=FALSE, message=FALSE, warning=FALSE}
set.seed(100390)
ros_apples <- ros(dfx$x, dfx$cen)

ros_df <- as.data.frame(ros_apples)

xdata <- rlnorm(10000, 1.58, 0.6)

makeSubData <- function(q){
  xcen <- xdata < qlnorm(q, 1.58,0.6)
  xdata[xcen] <- qlnorm(q, 1.58,0.6)
  fullsub <- apply(matrix(xdata, ncol=100), 2, mean)

  xdata[xcen] <- 0.5 * qlnorm(q, 1.58,0.6)
  halfsub <- apply(matrix(xdata, ncol=100), 2, mean)
  
  
  xdata[xcen] <-qlnorm(q, 1.58,0.6)/sqrt(2)
  sqrtsub <- apply(matrix(xdata, ncol=100), 2, mean)
  
  xdata[xcen] <-0
  zsub <- apply(matrix(xdata, ncol=100), 2, mean)
  
 dfx <- gather(data.frame(fullsub, halfsub, sqrtsub, zsub))
 dfx$q <- q
return(dfx)
 
}
xcen25 <- makeSubData(q=0.25)
xcen50 <- makeSubData(q=0.50)
xcen75 <- makeSubData(q=0.75)

xcenData <- bind_rows(list(xcen25, xcen50, xcen75))

xcenData$key <- factor(xcenData$key)
levels(xcenData$key) <- c("1 %*% DL", "1/2 %*% DL", 
                          "1/sqrt(2) %*% DL", "0 %*% DL")

ggplot(xcenData, aes(x=key, y=value, fill=factor(q))) + geom_boxplot() + theme_bw() + geom_hline(aes(yintercept = exp(1.58 + 0.5 * 0.6^2)), lty=2) + xlab("Subsitution Method") + ylab(expression(bar(x))) +scale_fill_manual("Censoring Quantile", values = c("grey95", "grey50", "grey25")) + scale_x_discrete(labels = c(expression(1 %*% DL), expression(DL/2), expression(DL/sqrt(2)), expression(0 %*% DL))) +
  labs(title = "Simulation of Sample Means by Censoring Quantile\n and Substitution Method", 
       subtitle = "Sample Size = 100",
caption = "True Mean = 5.8\n Censoring quantile is unitless [1 = 100% nondetect]")+
  theme(axis.text=element_text(size=10),
        axis.title=element_text(size=10,face="bold"),
        plot.caption=element_text(size=8))

```
<article>

We see that in simulation of a hundred random samples of size 100 from a known distribution, the results of our samples tend to vary by substitution method and by censoring rate.

Which method is “correct”? What do we do if our results are sensitive to how we treated nondetect data? Should any of our estimates of mean and standard deviation be dependent on the rate of nondetected data?

<\article>

## Learning Objectives
<article> 
From one thing, know ten thousand things. As Miyamoto Musashi said, I will use one idea - the notion of distribution functions to show 3 common ways of dealing with nondetect data work at the mathematical level.  Understanding the mathematical basis of these methods will enable you to correctly choose the optimal method of incorporating nondetect data into your analysis of environmental data.

So our very compact agenda will be:
<\article>

1. Define censored data
2. Describe the density functions for a distribution
  - Probability density function
  - Cumulative distribution function (and its mirror, the Survival Function)
  - How both can be used to calculate a mean
3. Utilization of Kaplan-Meier estimator for calculating mean
4. Robust Regression on Order Statistics for imputation of censored data
5. Maximum Likelihood Estimation


# Censoring

## Types of Censoring
Censoring is a condition where information is partially known. Censored data can contain a mixure of:

- Left censoring - data are reported as less than some value
- Right censoring - data are reported as greater than some value
- Interval censoring - where data are known to be between a lower and upper value


# Density Functions
<article>
The underlying functions that describe the distribution of the data are very powerful. A basic understanding of these equations and their relationship to the sample data, and the sample statistics they estimate, is the one thing that leads to ten thousands things in statistics. After this section, we could move towards discussions of confidence intervals, bootstrapping, central limit theorem, distributional fit testing, and regression. Today, its censoring. 
<\article>

## Probability Density Functions
Normal Probability Density Function (PDF) ($\mu=$ true mean,  $\sigma=$ true standard deviation)

$$
f(x) = \frac{1}{\sqrt{2\pi\sigma^2}}e^{-(x-\mu)^2/(2\sigma^2)}
$$

```{r pdf, echo=FALSE, warning=FALSE, message=FALSE, fig.height=5}
require("tidyverse")
.x  <- seq(-6, 6, length.out = 100)

dframe <- data.frame(x = .x, y = dnorm (.x, 0, 0.5), y1 = dnorm (.x, 0, 1), y2 = dnorm(.x, 0, 2))
dframe <- gather(dframe, key, value, -x)
dframe$key <- factor(dframe$key)

levels(dframe$key) <-  c("0.5", "1", "2")
.plot <- ggplot(dframe, aes(x = x, y = value, linetype=key)) + 
  geom_line(size = 1) + 
  scale_y_continuous(expand = c(0.01, 0)) + 
  xlab("x") + 
  ylab("Density") + 
  labs(title = "Normal Density Functions", 
       caption = "Normal PDF for Standard Deviation 0.5, 1 and 2") + 
  scale_linetype("Standard Deviation") + theme_bw(base_family = "sans")
print(.plot)


```

<article>
Recall from your basic statistics that the location and dispersion of the normal density curve is controlled entirely by 2 “parameters” (para means besides, meter means measurement), the mean and the standard deviation (or variance, which is the square of the standard deviation). 

The PDF is a function that indicates how likely a given value is. Recall that the area under a PDF is always equal to one as the function represents all possible values from a population. The higher the curve, the more likely the value is relative to other values. In the normal distribution equation, sigma determines the height of the curve at the mean.

You may be concerned that we are not venturing into log-normal or gamma distributions, since these are often used to model environmental populations. To keep things relatively simple, I am using the normal distribution to start out with. The lognormal distributions and gamma distributions have different PDF’s, and slightly different parameters. But conceptually, they work the same way.
     
<\article>

## PDF's Are Used to Calculate the Probability Ranges of Values



<article>
Specifically, a PDF can be used to calculate the probability that a single observation (usually this is written as 𝑋), is less than some specified value, say 0 in case A below. It can also be used for calculating probability it is greater than 0 (case B), or between -1, and 1 (Case C). Case D is the probability of X between -3.5 and 1. What is the probability of X = 1? Ha! It’s limit is zero.

PDF’s by definition always have an area under the curve equal to exactly 1.

<\article>
 
```{r makeless, echo=FALSE}

makepdf <- function(x = -3.5 , y =-1, title = "A. Probability (x<-1) =~16%"){
  ggplot(data.frame(x = c(-3.5, 3.5)), aes(x)) +
  stat_function(fun = dnorm) + 
  stat_function(fun = dnorm, 
                xlim = c(x,y),
                geom = "area", alpha =0.5)  +
                ggtitle(title) + ylab("Density") +
  theme_bw(base_size = 14, base_family = "sans")
}

pdfplots <- list(NULL)
pdfplots[[1]]<- makepdf()
pdfplots[[2]] <- makepdf(x = 0, y = 3.5, title = "B. Probability (x>0)  = 50%")
pdfplots[[3]] <- makepdf(x=-1,1, title = "C. Probability (-1<x<1) ~ 68.2%")
pdfplots[[4]] <- makepdf(x=-3.5,1, title = "C. Probability (-3.5<x<1) ~ 84.1%")


xplot <- gridExtra::grid.arrange(grobs = pdfplots, nrow=2, ncol=2)

```

## Normal Probability Plot 
```{r, qq1, echo=FALSE, message = FALSE}
set.seed(2)
ydata <- data.frame(sample = c(rep("Sample 1", 100),
                               rep("Sample 2", 100),
                               rep("Sample 3", 100),
                               rep("Sample 4", 100)),
                               x=rnorm(400))
ydata <- ydata[order(ydata$sample, ydata$x),]
ydata$order <- 1:100

ydata$highlight = ifelse(ydata$order %in% c(2,16,50,84,98), "#FF0000", "#000000")
ydata$order_txt = ifelse(ydata$order %in% c(2,16,50,84,98), ydata$order, "")

ggplot(ydata, aes(sample=x)) + 
  geom_point(color=ydata$highlight, stat="qq", size = 0.65)  +
  geom_text(label=ydata$order_txt, stat="qq", nudge_y=1, color = "red") +
  stat_qqline(alpha =0.5) + facet_wrap(~sample) + 
  scale_x_continuous(breaks = -2:2, labels = function(x) paste0(x, " \n [",100 * signif(pnorm(-2:2, lower.tail=FALSE),2), "%]")) + 
  theme_bw(base_family = "sans") +
  labs(
    title = "Four Samples of 100 Observations From Normal Distribution",
    caption = "4 Samples of n = 100 from Normal Distribution \nNumbers indicate order of value",
    subtitle = "Compared to a normal distribution model",
    y = "Sample Value",x = "Theoretical Quantile\n[%Exceeding]")
```

<article>
There is a relationship between the quantiles and the standard deviation in the normal distribution. We would expect about 84% of the data to be less than the mean plus 1 standard deviation. 16% of the data to be less than the mean minus 1 standard deviation. Practically, we can use this relationship to assess how well the normal model fit a sample of data.

It’s easier to illustrate - if we collect 4 sample sets of n=100 from a normal population, and order the observations from the lowest value to the highest value. Plotted with the X axis representing the standard deviations from the mean, this plot will reveal a linear relationship between the observed and theoretical quantiles, with the standard deviation equal to 84th minus the 50th observation.

<\article>


## Normal Probability Plot - Lognormal Data  

```{r, qq2, echo=FALSE, message = FALSE}
set.seed(2)
y1data <- data.frame(sample = c(rep("Sample 1", 100),
                               rep("Sample 2", 100),
                               rep("Sample 3", 100),
                               rep("Sample 4", 100)),
                               x=rlnorm(400, sdlog = 0.75))
y1data <- y1data[order(y1data$sample, y1data$x),]
y1data$order <- 1:100

y1data$highlight = ifelse(y1data$order %in% c(2,16,50,84,98), "#FF0000", "#000000")
y1data$order_txt = ifelse(y1data$order %in% c(2,16,50,84,98), ydata$order, "")

ggplot(y1data, aes(sample=x)) + 
  geom_point(color=y1data$highlight, stat="qq", size = 0.65)  +
  geom_text(label=y1data$order_txt, stat="qq", nudge_y=1, color = "red") +
  stat_qqline(alpha = 0.5) + facet_wrap(~sample) + 
  scale_x_continuous(breaks = -2:2, labels = function(x) paste0(x, " \n [",100 * signif(pnorm(-2:2, lower.tail=FALSE),2), "%]")) + 
  theme_bw(base_family = "sans") + 
  labs(
    title = "Four Samples of 100 Observations From Lognormal Distribution",
    subtitle = "Compared to normal distribution model",
    caption = "4 Samples of n = 100 from Lognormal Distribution \nNumbers indicate order of value",
    y = "Sample Value",x = "Theoretical Quantile\n[%Exceeding]")
```


<article>
If the data were samples from say a different distribution, say a lognormal distribution, then the relationship between the sample and theoretical quantiles will not be linear.
<\article>

    
## PDFs and Random Frequency of Values in Sample

PDF's can be used to predict frequency of random samples falling within a certain range.

```{r, pdffrq, echo=FALSE, message = FALSE, warning =FALSE}

ydata_dist <- ydata %>% group_by(sample) %>%
  summarise(mean = mean(x), sd = sd(x))

dfx <- split(ydata_dist, ydata_dist$sample) %>%
  lapply(function(x){
    data.frame(sample = x$sample, x = seq(from = -4.5, to = 4.5, length = 100), `Population` = dnorm(x = seq(from = -4.5, to = 4.5, length = 100)),
    `Sample Fit` = dnorm(x = seq(from = -4.5, to = 4.5, length = 100), mean = x$mean, sd=x$sd))
  }) %>%
  bind_rows(.) 

lx <- split(ydata, ydata$sample) %>% lapply(function(x) hist(x$x, plot=FALSE,
                                                       breaks = c(-3,-2.5, -2.0, -1.5, -1.0, -0.5,  0.0,  0.5,  1.0,  1.5,  2.0,  2.5,3)))

lx <- lapply(lx, function(x){
  left = x$breaks[1:length(x$breaks)-1]
  right = x$breaks[2:length(x$breaks)]
  count = x$counts
  
  left <- rep(left, count)
  right <- rep(right, count)
  
  censfit<- fitdistrplus::fitdistcens(censdata = data.frame(left=left, right=right), distr="norm")

  data.frame(x = seq(-4.5, 4.5, length = 100), `Fit Grouped` = dnorm(x=seq(-4.5,4.5, length = 100), mean = censfit$estimate[[1]], sd = censfit$estimate[[2]] ))
  
})

for(i in 1:length(lx)){
  lx[[i]]$sample <- names(lx)[[i]]
}

dfx <- full_join(dfx, bind_rows(lx)) %>%
  gather(Model, dens, Population, Sample.Fit, Fit.Grouped)

dfx$Model <- factor(dfx$Model, levels = c("Population", "Sample.Fit", "Fit.Grouped"))

levels(dfx$Model) <- c("Population","Sample","Binned Data")


ggplot(ydata, aes(x=x)) + geom_histogram(aes(y=..density..),
                                         breaks = c(-3.0, -2.5, -2.0, -1.5, -1.0, -0.5,  0.0,  0.5,  1.0,  1.5,  2.0,  2.5,3), alpha = 0.5) + facet_wrap(~sample) +
  xlim(-4.5,4.5) + theme_bw(base_family = "sans") +
    geom_line(data = dfx, aes(y=dens, color = Model), size=1.25) + scale_color_manual("Model", values = c("#000000", "#0072B2", "#F0E442")) +
  labs(title = "Four Samples of 100 Observations From Normal Distribution", 
       caption = "Parameters: Mean 0, Standard Deviation 1\nBlack line indicated theoretical distribution",
       y = "Frequency")


```



## PDF's and the Arithmetic Mean of a Distribution

The arithmetic mean value of a distribution is equal to the sum of x multiplied by x's frequency of observation. Since the PDF is approaches zero for any single value, we have to use calculus to represent this:

$$
\mu = \int_{-\infty}^\infty xf(x) dx
$$
It follows then that in a simple random sample of size $n$, each sample observation has a frequency of observation of $\frac{1}{n}$. The sample mean is:  $$\bar{x} = \frac{\sum_{i=1}^{n}x_i}{n}$$.

## Cumulative Distribution Functions

- The cumulative distribution function (CDF) is the probability that some value $X$ is less than or equal to a given value ($x$).

```{r cdf1, echo=FALSE, fig.height=5}
pdf1 <- ggplot(data.frame(x = c(-3.5, 3.5)), aes(x)) +
  stat_function(fun = dnorm) + 
  stat_function(fun = dnorm, 
                xlim = c(-3.5,-2),
                geom = "area", alpha =0.1)  +
    stat_function(fun = dnorm, 
                xlim = c(-3.5,-1),
                geom = "area", alpha =0.1) +
    stat_function(fun = dnorm, 
                xlim = c(-3.5,0),
                geom = "area", alpha =0.1) +
    stat_function(fun = dnorm, 
                xlim = c(-3.5,1),
                geom = "area", alpha =0.1) +
    stat_function(fun = dnorm, 
                xlim = c(-3.5,2),
                geom = "area", alpha =0.1) +
  stat_function(fun = dnorm, 
                xlim = c(-3.5,3.5),
                geom = "area", alpha =0.1) +
                ggtitle("Normal PDF") + ylab("Density") +
  theme_bw(base_family = "sans")



cdf1 <- ggplot(data.frame(x = c(-3.5, 3.5)), aes(x)) +
  stat_function(fun = pnorm) + 
  stat_function(fun = pnorm, 
                xlim = c(-3.5,-2),
                geom = "area", alpha =0.1)  +
    stat_function(fun = pnorm, 
                xlim = c(-3.5,-1),
                geom = "area", alpha =0.1) +
    stat_function(fun = pnorm, 
                xlim = c(-3.5,0),
                geom = "area", alpha =0.1) +
    stat_function(fun = pnorm, 
                xlim = c(-3.5,1),
                geom = "area", alpha =0.1) +
    stat_function(fun = pnorm, 
                xlim = c(-3.5,2),
                geom = "area", alpha =0.1) +
  stat_function(fun = pnorm, 
                xlim = c(-3.5,3.5),
                geom = "area", alpha =0.1) +
                ggtitle("Normal CDF") + ylab(expression("P(x<X)")) +
  theme_bw(base_family = "sans")



xplot <- gridExtra::grid.arrange(grobs = list(pdf1, cdf1), nrow=2, ncol=1)

```




## Survivor Functions

- The survivor function is 1 - CDF
- The area under the survivor function is the arithmetic mean


```{r sdf1, echo=FALSE, message=FALSE, warning=FALSE}
(sdf1 <- ggplot(data.frame(x = c(-3.5, 3.5)), aes(x)) +
  stat_function(fun = pnorm, args = list(lower.tail=FALSE)) + 
  stat_function(fun = pnorm, args = list(lower.tail=FALSE), 
                xlim = c(-3.5,-2),
                geom = "area", alpha =0.1)  +
    stat_function(fun = pnorm, args = list(lower.tail=FALSE), 
                xlim = c(-3.5,-1),
                geom = "area", alpha =0.1) +
    stat_function(fun = pnorm, args = list(lower.tail=FALSE), 
                xlim = c(-3.5,0),
                geom = "area", alpha =0.1) +
    stat_function(fun = pnorm, args = list(lower.tail=FALSE), 
                xlim = c(-3.5,1),
                geom = "area", alpha =0.1) +
    stat_function(fun = pnorm, args = list(lower.tail=FALSE), 
                xlim = c(-3.5,2),
                geom = "area", alpha =0.1) +
  stat_function(fun = pnorm, args = list(lower.tail=FALSE), 
                xlim = c(-3.5,3.5),
                geom = "area", alpha =0.1) +
                ggtitle("Normal Survivor Function") + ylab(expression("P(x>X)")) +
  theme_bw(base_family = "sans"))

```


## Empirical Distribution Function

- Used for sample data
- The empirical distribution function is like the CDF, but function step $1/n$ at each of the $n$ data points.

```{r ecdf, echo=FALSE, fig.height=5}
ggplot(ydata, aes(x, group=sample, col=sample)) + geom_line(stat="ecdf") +
  stat_function(fun=pnorm, col="black", size=1.1) +
  theme_bw(base_family = "sans") + scale_color_brewer("Sample Group", type = "div") + labs(title = "Empirical CDF (4 Samples) \n and Theoretical ECF",
                                                                                           subtitle = "100 samples from N(0,1) distribution") + ylab(expression("P(X<x)"))
```

## Calculating a Sample Mean from a Empirical Survivor Function (1)

Jim would like to see if his coworkers on his floor would like to buy a fancy coffee machine for the office breakroom. He randomly polls 10 persons of the 100 persons on the floor and asks them how much they would contribute for a coffee machine. 

He gets the following data:  

```{r coffee, echo=FALSE}
dfx <- data.frame(Name = c("Brian", "Mike", "Amy", "Charles", "Rick", "Susan", "Rose", "Elaine", "Sharon", "John"),
           `Dollar Contribution` = 1:10, check.names=FALSE)
  knitr::kable(dfx)
```

## Calculating a Sample Mean from a Empirical Survivor Function (2)

- You can get the sample sum by multiplying each incremental dollar contribution by the number of people who are willing to give:

```{r coffee2, echo=FALSE, warning = FALSE}
dfx <- data.frame(`Dollar Contribution`= 0:10, Contrib = 10:0, check.names = FALSE)
ggplot(dfx, aes(x=`Dollar Contribution`, y=`Contrib`)) + geom_step() + scale_y_continuous(breaks = 1:10) +
  scale_x_continuous(breaks = 0:10) + 
  labs(y = "Number Willing to Give \n(at least)",
       x = "Dollar Amount") + theme_bw() +
  annotate("text", x = 0.5:9.5, y =10:1 * 0.5, label = paste(10:1, "x", rep(1,10))) +
    geom_path(data=data.frame(x = rep(0:10, each=2), y = c(rbind(rep(0,10),10:0 )), group = rep(0:10,each = 2) ), aes(x=x, y=y, group=group), col = "grey", alpha = .70) + geom_hline(yintercept = 0) +
  annotate("text", x = 6.5, y = 6.5, label = "sum(x)==55",parse = TRUE)
```
<article>
Survivor functions can estimate the total amount. Here we see that 10 people are willing to give 1 dollar, 9 people are willing to give one more dollar after that, 8 people are willing to give a dollar after that, and so on until the one person has given 10 dollars. We can sum this amount and get 55 dollars. Divided by our sample size, that equates to a sample mean of 5.5 dollars.

This is not any different than taking the sum of each of our observations and dividing by the sample size. So why would we go about it this way?
<\article>

## Calculating a Sample Mean from a Empirical Survivor Function (3)

```{r coffee3, echo=FALSE, warning = FALSE}

ggplot(dfx, aes(x=`Dollar Contribution`, y=`Contrib`/10)) + geom_step() + scale_y_continuous(breaks = 1:10/10) +
  scale_x_continuous(breaks = 0:10) + 
  labs(y = "S(x)",
       x = "Dollar Amount") + theme_bw() +
  annotate("text", x = 0.5:9.5, y =10:1/10 * 0.5, label = paste((10:1)/10, "x", rep(1,10))) +
    geom_path(data=data.frame(x = rep(0:10, each=2), y = c(rbind(rep(0,10)/10,10:0/10 )), group = rep(0:10,each = 2) ), aes(x=x, y=y, group=group), col = "grey", alpha = .70) + geom_hline(yintercept = 0) +
  annotate("text", x = 6.5, y = .65, label = "bar(x)==sum(S(x))%*%Delta~x==5.5",parse = TRUE)
```
<article>
In fact, we can go ahead, and through the distributive property of division, create an empirical survival function by dividing by our sample size at each dollar amount.

This is still not any different than taking the sum of each of our observations and dividing by the sample size. So why would we go about it this way?
<\article>

## Product Limit Estimation
The probability of exceeding a given value of $x$, is the product sequence all of the lower values.

For instance, to get a value of "3" or more, the sequence would be:
$\frac{10}{10} * \frac{9}{10} * \frac{8}{9} = \frac{8}{10}$.

- All of the above methods are the same mathematically as sum of the observations divided by the sample size, $\frac{\sum_{i=1}^{n}x_i}{n}$.

- Why would I do it that way?

# Kaplan-Meier

## Because of Charles!

- Charles gives us an answer of, " I will give you no more than 4 dollars!"
- If we assume that Charles would give any amount between 0 and 3 with equal probability, we could assign that extra (1/3) probability to the observations below 4 by multiplying the ECDF at those values by  
$1 + \frac{1}{3}$.

- So the ECDF probability at 1 was $\frac{1}{10}$ without Charlie's censoring
- It becomes $\frac{1 \times (1+\frac{1}{3})}{10}\approx0.133$
- At 2 it become $\frac{2 \times (1+\frac{1}{3})}{10}\approx0.267$
- Empirical Survival Curve and Mean are estimated in usual manner.

## Emperical Survival for Coffee Data
(Charles Censored and Uncensored) -- need to think about this.
```{r echo=FALSE, warn=FALSE, message=FALSE}
dfy <- dfx

dfy$Contrib[1:4] <- 10 - ((10 - dfy$Contrib)[1:4] * (1 + 1/3))
dfy <- dfy[-c(5),] # GODDBYE CHARLES!

dfy$Censored <-"Charles (<4)"


dfx$Censored <- "No Censoring"

dfz <- bind_rows(dfy, dfx)

ggplot(dfz, aes(x=`Dollar Contribution`, y=`Contrib`/10, color = Censored)) + geom_step(size=1.25) + 
  scale_y_continuous(breaks = 1:10/10) +
  scale_x_continuous(breaks = 0:10) + 
  labs(y = "S(x)",
       x = "Dollar Amount") + theme_bw() +
    geom_path(
      data=data.frame(x = rep(0:10, each=2), 
                      y = c(rbind(rep(0,10)/10,10:0/10 )), 
                      group = rep(0:10,each = 2) ), 
      aes(x=x, y=y, group=group), col = "grey", alpha = .70) +
  geom_hline(yintercept = 0) + 
  scale_color_manual(name = "Data", values = c("#0072B2", "#F0E442")) +
  annotate("text", x = c(0.5:2.5, 4, 5.5:9.5), y =9:1/10 * 0.5, 
           label = paste(signif(dfy$Contrib[1:9]/10,2), 
                         c(rep(1,3), 2, rep(1,5)), sep="x")) +
  annotate("text", x = 6.5, y = .65, label = "bar(x)==sum(S(x))%*%Delta~x==5.3",parse = TRUE)



       

```
<article>


The survival probabilities have decreased for levels below the censoring level. - The change in X (𝐷𝑒𝑙𝑡𝑎) is 2 between 3 and 5. Some key observations:

1. Kaplan-Meier is adjusting the probabilities of observed values based on the order the censoring occurs. The effect will change depending on how high a censored value you have. So if you had a <10 observation – your sample mean would then become 5 (effectively, you have dropped your censored observation from the data set).
2. The mean from the Kaplan-Meier estimate is somewhat intransient to deviations in censoring levels between observed observations. For instance, if Charles has said, “ I will give less than $4.75!” We will have gotten the same sample mean (\$5.3).
3. Data where there are no uncensored observations below the censoring levels (singly censored data) will effectively result in substitution in the probabilities getting stacked at the lowest censoring level.This results in almost an identical estimate of the mean as if you substituted the detection limit.

<\article>

## Robust Regression on Order Statistics

- Uses the linear relationship between the data values and their normal scores to create imputed values for the nondetected values.
- This is like substitution, but with a range of values instead of a single value.
- If applicable, transformation functions can be used to make skewed data normal (e.g. log function for lognormal data). 
- Imputed data are transformed back to original scale.
- Observed and imputed values are combined to create summary statistics. 

## Imputing Values using ROS

```{r, echo = FALSE, warning = FALSE, message=FALSE}
rosMean <- function(q){
  xcen <- xdata < qlnorm(q, 1.58,0.6)
  xdata[xcen] <- qlnorm(q, 1.58,0.6)
  
  xxdata <- matrix(xdata, ncol = 100)
  xxcen <- matrix(xcen, ncol = 100)
  
  meanx = unlist(lapply(1:100, function(i){
  mean(ros(xxdata[,i], xxcen[,i]))  
  }))
  
  #browser()
  data.frame(meanx, q = rep(q,100))
}

xxx <- (lapply(c(0.25,0.5,0.75), rosMean) %>%
  bind_rows())


set.seed(2)
q <- rlnorm(20)
qcen <- q < 1
q[qcen] <- 1

rosObj <- ros(q,qcen)

xdf <- data.frame(rosObj) 

xdf$q <- qnorm(xdf$pp)

xdf$mymodel <- predict(rosObj, xdf$q)

xdf %>%
  mutate(q = qnorm(pp), censored = ifelse(censored, "ROS Imputed", "Observed Data")) %>%
  ggplot(aes(x = q, y = modeled, shape=censored)) + geom_point(size = 2) +
  scale_y_continuous(trans = "log", breaks = 1:10) + 
  scale_x_continuous(
    labels = function(x) paste0(x, " \n [",100 * signif(pnorm(x, lower.tail=FALSE),2), "%]")) +
  labs(x = "Normal Quantiles\n[% Exceeding]", y = "Sample Value") +
  scale_shape_manual("Data Type", values = c(19,12)) + 
  geom_line(aes(y = mymodel, group = NA)) +
  theme_bw()

```

<article>
ROS imputes the values based on the ranks of the censored data.
This usually works well, but sometimes if the distribution is contaminated with another distribution, or if the censoring limits are not precise, it is possible for imputed values to be higher than the censoring limit. It is always a good idea to check the linearity of the relationship of the quantiles of the assumed model to the values when using this method.

<\article>


## Simulation of ROS Methods
```{r simROS, echo=FALSE}

ggplot(xxx, aes(x=q, y=meanx, fill=factor(q))) + geom_boxplot() + theme_bw() + geom_hline(aes(yintercept = exp(1.58 + 0.5 * 0.6^2)), lty=2) + xlab("Subsitution Method") + ylab(expression(bar(x))) +scale_fill_manual("Censoring Quantile", values = c("grey95", "grey50", "grey25")) + 
  labs(title = "Simulation of Sample Means by Censoring Quantile\n using Robust Regression on Order Statistics", 
       subtitle = "Sample Size = 100",
caption = "True Mean = 5.8\n Censoring Quantile is unitless [1 = 100% nondetect]",
x = "Censoring Quantile")+ ylim(c(0,9)) +
  theme(axis.text=element_text(size=10),
        axis.title=element_text(size=10,face="bold"),
        plot.caption=element_text(size=8))


```

Recalling our simulation of censored data from the start of our talk, substitution biased estimates of the mean, with the effect being worse for higher substitution. With ROS, bias is kept moderately low, even at very high censoring rates.   

# MLE

```{r MLE}
library(LearnBayes)
 d = mycontour(normchi2post, c(220, 330, 500, 9000), time,
     xlab="mean",ylab="variance")


>These things cannot be explained in detail. From one thing, know ten thousand things. You must study hard.

- Miyamoto Musashi, _Go Rin No Sho_



# Introduction - why does this matter?

## Example Data - Arsenic in Apple Juice
```{r htmltable, echo=FALSE, message=FALSE, warning=FALSE}
require(htmltab)
require(tidyverse)
require(magrittr)
require(NADA)
require(gtable)

qq.line <- function(data, qf, na.rm) {
    # from stackoverflow.com/a/4357932/1346276
    q.sample <- quantile(data, c(0.25, 0.75), na.rm = na.rm)
    q.theory <- qf(c(0.25, 0.75))
    slope <- diff(q.sample) / diff(q.theory)
    intercept <- q.sample[1] - slope * q.theory[1]

    list(slope = slope, intercept = intercept)
}

StatQQLine <- ggproto("StatQQLine", Stat,
    # http://docs.ggplot2.org/current/vignettes/extending-ggplot2.html
    # https://github.com/hadley/ggplot2/blob/master/R/stat-qq.r

    required_aes = c('sample'),

    compute_group = function(data, scales,
                             distribution = stats::qnorm,
                             dparams = list(),
                             na.rm = FALSE) {
        qf <- function(p) do.call(distribution, c(list(p = p), dparams))

        n <- length(data$sample)
        theoretical <- qf(stats::ppoints(n))
        qq <- qq.line(data$sample, qf = qf, na.rm = na.rm)
        line <- qq$intercept + theoretical * qq$slope

        data.frame(x = theoretical, y = line)
    } 
)

stat_qqline <- function(mapping = NULL, data = NULL, geom = "line",
                        position = "identity", ...,
                        distribution = stats::qnorm,
                        dparams = list(),
                        na.rm = FALSE,
                        show.legend = NA, 
                        inherit.aes = TRUE) {
    layer(stat = StatQQLine, data = data, mapping = mapping, geom = geom,
          position = position, show.legend = show.legend, inherit.aes = inherit.aes,
          params = list(distribution = distribution,
                        dparams = dparams,
                        na.rm = na.rm, ...))
}


url <- "https://www.fda.gov/Food/FoodborneIllnessContaminants/Metals/ucm283725.htm"
juice <- htmltab(doc = url)
x <- as.numeric(as.character(juice[,3]))
x[x==0] <- 0.25
cen <- is.na(x)
xhalf <- x
xsqrt <- x
xzero <- x
x[cen] <- 2
xhalf[cen] <- 1
xsqrt[cen] <- 2/sqrt(2)
cen[x==0.25] <- TRUE
xhalf[x==0.25] <- 0.0
xsqrt[x==0.25] <- 0.0
xzero[cen] <- 0

dfx <- data.frame(x=x, xhalf = xhalf, xsqrt = xsqrt, xzero = xzero, cen=cen)
# 
# myx <- function(x) x
# 
# ros_apples <- ros(dfx$x, dfx$cen, forwardT="myx", reverseT="myx")

# ros_df <- as.data.frame(ros_apples)

myapples <- gather(dfx, key = "key", value = "obs", x, xhalf, xsqrt, xzero) %>%
  mutate( key = recode(key, x = "1 %*% DL", xhalf = "DL/2", xsqrt = "DL/sqrt(2)", xzero = "0%*%DL")) 

sumapples <- myapples %>%
  group_by(key) %>%
  summarise(average = signif(mean(obs),3),  `standard deviation` = signif(sd(obs),3)) 

saveRDS(myapples, file= "./apples.rds")

ggplot(myapples, aes(x=obs, fill = !cen)) +geom_histogram() + 
  facet_wrap(~key, labeller = label_parsed) + 
  scale_fill_manual("Detected", values = c("Grey", "Black")) +
  geom_vline(data=sumapples, aes( xintercept = average, color = "red", linetype = "dashed"), show.legend = FALSE) +
  xlab("Inorganic Arsenic Concentration [ppb]") + geom_text(data = sumapples, x = 6, y =18, aes(label = paste("bar(x)==", average)), parse = TRUE, inherit.aes = FALSE) +
  geom_text(data = sumapples, x = 6, y =12, aes(label = paste("s ==", `standard deviation`)), parse = TRUE, inherit.aes = FALSE) + 
  labs(title = "Inorganic Arsenic Concentration in Apple Juice", 
       subtitle = "2011 FDA Survey [n=94]", 
       caption = "Source:FDA. Results of Arsenic Analysis in Single-Strength Apple Juice, 2011\n (ORA Sampling Assignment 2011102701).") +
  theme_bw() +
  theme(axis.text=element_text(size=10),
        axis.title=element_text(size=10,face="bold"),
        plot.caption=element_text(size=8))

```

 
<article>
 
- About 28% of these data are below detection limit
- Mean varies by up to 0.5 ppb
- Standard Deviation varies by up to 0.66 ppb

What this means is there is variability in our answer based on how we selected our method of imputing non-detected data. The amount of variability increases with the amount of nondetected data. 

Which method is "correct"? 
What do we do if our results are sensitive to how we treated nondetect data?
Should any of our estimates of mean and standard deviation be dependent on the rate of nondetected data?


<\article>

## Simulation of Substitution Methods
```{r, echo=FALSE, message=FALSE, warning=FALSE}
set.seed(100390)
ros_apples <- ros(dfx$x, dfx$cen)

ros_df <- as.data.frame(ros_apples)

xdata <- rlnorm(10000, 1.58, 0.6)

makeSubData <- function(q){
  xcen <- xdata < qlnorm(q, 1.58,0.6)
  xdata[xcen] <- qlnorm(q, 1.58,0.6)
  fullsub <- apply(matrix(xdata, ncol=100), 2, mean)

  xdata[xcen] <- 0.5 * qlnorm(q, 1.58,0.6)
  halfsub <- apply(matrix(xdata, ncol=100), 2, mean)
  
  
  xdata[xcen] <-qlnorm(q, 1.58,0.6)/sqrt(2)
  sqrtsub <- apply(matrix(xdata, ncol=100), 2, mean)
  
  xdata[xcen] <-0
  zsub <- apply(matrix(xdata, ncol=100), 2, mean)
  
 dfx <- gather(data.frame(fullsub, halfsub, sqrtsub, zsub))
 dfx$q <- q
return(dfx)
 
}
xcen25 <- makeSubData(q=0.25)
xcen50 <- makeSubData(q=0.50)
xcen75 <- makeSubData(q=0.75)

xcenData <- bind_rows(list(xcen25, xcen50, xcen75))

xcenData$key <- factor(xcenData$key)
levels(xcenData$key) <- c("1 %*% DL", "1/2 %*% DL", 
                          "1/sqrt(2) %*% DL", "0 %*% DL")

ggplot(xcenData, aes(x=key, y=value, fill=factor(q))) + geom_boxplot() + theme_bw() + geom_hline(aes(yintercept = exp(1.58 + 0.5 * 0.6^2)), lty=2) + xlab("Subsitution Method") + ylab(expression(bar(x))) +scale_fill_manual("Censoring Rate", values = c("grey95", "grey50", "grey25")) + scale_x_discrete(labels = c(expression(1 %*% DL), expression(DL/2), expression(DL/sqrt(2)), expression(0 %*% DL))) +
  labs(title = "Simulation of Sample Means by Censoring Rate\n and Substitution Method", 
       subtitle = "Sample Size = 100",
caption = "True Mean = 5.8\n Censoring rate is unitless [1 = 100% nondetect]")+
  theme(axis.text=element_text(size=10),
        axis.title=element_text(size=10,face="bold"),
        plot.caption=element_text(size=8))

```
<article>
We see that in simulation of a hundred random samples of size 100 from a known distribution, the results of our samples tend to vary by substitution method and by censoring rate.
<\article>

## Learning Objectives
<article> 
From one thing, know ten thousand things. As Miyamoto Musashi said, I will use one idea - the notion of distribution functions to show 3 common ways of dealing with nondetect data work at the mathematical level.  Understanding the mathematical basis of these methods will enable you to correctly choose the optimal method of incorporating nondetect data into your analysis of environmental data.

So our very compact agenda will be:
<\article>

1. Define censored data
2. Describe the density functions for a distribution
  - Probability density function
  - Cumulative distribution function (and its mirror, the Survival Function)
  - How both can be used to calculate a mean
3. Utilization of Kaplan-Meier curve for calculating mean
4. Robust Regression on Order Statistics for imputation of censored data
5. Maximum Likelihood Estimation


# Censoring

## Types of Censoring
Censoring is a condition where information is partially known. Censored data can contain a mixure of:

- Left censoring - data are reported as less than some value
- Right censoring - data are reported as greater than some value
- Interval censoring - where data are known to be between a lower and upper value


# Density Functions
<article>
The underlying functions that describe the distribution of the data are very powerful. A basic understanding of these equations and their relationship to the sample data, and the sample statistics they estimate, is the one thing that leads to ten thousands things in statistics. After this section, we could move towards discussions of confidence intervals, bootstrapping, central limit theorem, distributional fit testing, and regression. Today, its censoring. 
<\article>

## Probability Density Functions
Normal Probability Density Function (PDF) ($\mu=$ true mean,  $\sigma=$ true standard deviation)

$$
f(x) = \frac{1}{\sqrt{2\pi\sigma^2}}e^{-(x-\mu)^2/(2\sigma^2)}
$$

```{r pdf, echo=FALSE, warning=FALSE, message=FALSE, fig.height=5}
require("tidyverse")
.x  <- seq(-6, 6, length.out = 100)

dframe <- data.frame(x = .x, y = dnorm (.x, 0, 0.5), y1 = dnorm (.x, 0, 1), y2 = dnorm(.x, 0, 2))
dframe <- gather(dframe, key, value, -x)
dframe$key <- factor(dframe$key)

levels(dframe$key) <-  c("N(0,0.5)", "N(0,1)", "N(0,2)")
.plot <- ggplot(dframe, aes(x = x, y = value, linetype=key)) + 
  geom_line(size = 1) + 
  scale_y_continuous(expand = c(0.01, 0)) + 
  xlab("x") + 
  ylab("Density") + 
  labs(title = "Normal Density Functions", 
       caption = "Normal PDF for Standard Deviation 0.5, 1 and 2") + 
  scale_linetype("Standard Deviation") + theme_bw(base_size = 14, base_family = "sans")
print(.plot)


```

<article>
Recall from your basic statistics that the location and dispersion of the normal density curve is controlled entirely by 2 "parameters", the mean and the standard deviation (or variance, which is the square of the standard deviation). The PDF is a function that defines the probabilty that a value is greater than a value, less than a value or between two values occurs. You may be concerned that we are not venturing into log-normal or gamma distributions, since these are often used to model environmental populations. However, we need to first master how distribution functions in general can be used for calculating summary statistics of censored data. The lognormal distributions and gamma distributions have different PDF's, and the concepts are transferable.      
<\article>

## PDF's Are Used to Calculate the Probability Ranges of Values



<article>
- Specifically, a PDF can be used to calculate the probability that a single observaion (usually this is written as $X$), is less than some specified value, say 0 in case A below. It can also be used for calculating probability it is greater than 0 (case B), or between -1, and 1 (Case C). Case D is the probability of X between -3.5 and 1. What is the probability of X = 1? Ha! It's limit is zero.

- PDF's by definition always have an area under the curve equal to exactly 1.

<\article>
 
```{r makeless, echo=FALSE}

makepdf <- function(x = -3.5 , y =-1, title = "A. Probability (x<-1) = 50%"){
  ggplot(data.frame(x = c(-3.5, 3.5)), aes(x)) +
  stat_function(fun = dnorm) + 
  stat_function(fun = dnorm, 
                xlim = c(x,y),
                geom = "area", alpha =0.5)  +
                ggtitle(title) + ylab("Density") +
  theme_bw(base_size = 14, base_family = "sans")
}

pdfplots <- list(NULL)
pdfplots[[1]]<- makepdf()
pdfplots[[2]] <- makepdf(x = 0, y = 3.5, title = "B. Probability (x>0) = 50%")
pdfplots[[3]] <- makepdf(x=-1,1, title = "C. Probability (-1<x<1) ~68.2%")
pdfplots[[4]] <- makepdf(x=-3.5,1, title = "C. Probability (-3.5<x<1) ~84.1%")


xplot <- gridExtra::grid.arrange(grobs = pdfplots, nrow=2, ncol=2)

```

## Normal Probability Plot 
```{r, qq1, echo=FALSE, message = FALSE}
set.seed(2)
ydata <- data.frame(sample = c(rep("Sample 1", 100),
                               rep("Sample 2", 100),
                               rep("Sample 3", 100),
                               rep("Sample 4", 100)),
                               x=rnorm(400))
ydata <- ydata[order(ydata$sample, ydata$x),]
ydata$order <- 1:100

ydata$highlight = ifelse(ydata$order %in% c(2,16,50,84,98), "#FF0000", "#000000")
ydata$order_txt = ifelse(ydata$order %in% c(2,16,50,84,98), ydata$order, "")

ggplot(ydata, aes(sample=x)) + 
  geom_point(color=ydata$highlight, stat="qq", size = 0.65)  +
  geom_text(label=ydata$order_txt, stat="qq", nudge_y=1, color = "red") +
  stat_qqline(alpha =0.5) + facet_wrap(~sample) + 
  scale_x_continuous(breaks = -2:2, labels = function(x) paste0(x, " \n [",100 * signif(pnorm(-2:2, lower.tail=FALSE),2), "%]")) + 
  theme_bw(base_family = "sans") +
  labs(
    title = "Four Samples of 100 Observations From Normal Distribution",
    caption = "4 Samples of n = 100 from Normal Distribution \nNumbers indicate order of value",
    subtitle = "Compared to a normal distribution model",
    y = "Sample Value",x = "Theoretical Quantile\n[%Exceeding]")
```

## Normal Probability Plot - Lognormal Data  

```{r, qq2, echo=FALSE, message = FALSE}
set.seed(2)
y1data <- data.frame(sample = c(rep("Sample 1", 100),
                               rep("Sample 2", 100),
                               rep("Sample 3", 100),
                               rep("Sample 4", 100)),
                               x=rlnorm(400, sdlog = 0.75))
y1data <- y1data[order(y1data$sample, y1data$x),]
y1data$order <- 1:100

y1data$highlight = ifelse(y1data$order %in% c(2,16,50,84,98), "#FF0000", "#000000")
y1data$order_txt = ifelse(y1data$order %in% c(2,16,50,84,98), ydata$order, "")

ggplot(y1data, aes(sample=x)) + 
  geom_point(color=y1data$highlight, stat="qq", size = 0.65)  +
  geom_text(label=y1data$order_txt, stat="qq", nudge_y=1, color = "red") +
  stat_qqline(alpha = 0.5) + facet_wrap(~sample) + 
  scale_x_continuous(breaks = -2:2, labels = function(x) paste0(x, " \n [",100 * signif(pnorm(-2:2, lower.tail=FALSE),2), "%]")) + 
  theme_bw(base_family = "sans") + 
  labs(
    title = "Four Samples of 100 Observations From Lognormal Distribution",
    subtitle = "Compared to normal distribution model",
    caption = "4 Samples of n = 100 from Lognormal Distribution \nNumbers indicate order of value",
    y = "Sample Value",x = "Theoretical Quantile\n[%Exceeding]")
```



    
## PDFs and Random Frequency of Values in Sample

PDF's can be used to predict frequency of random samples falling within a certain range.

```{r, pdffrq, echo=FALSE, message = FALSE}



ggplot(ydata, aes(x=x)) + geom_histogram(aes(y=..density..),binwidth = 0.5, alpha = 0.5) + facet_wrap(~sample) +
  xlim(-4.5,4.5) + theme_bw(base_family = "sans") +
  stat_function(fun = dnorm) + 
  stat_function(fun = dnorm, 
                xlim = c(-4.5,4.5),
                geom = "line", alpha =0.5) +
  labs(title = "Four Samples of 100 Observations From Normal Distribution", 
       caption = "Parameters: Mean 0, Standard Deviation 1\nBlack line indicated theoretical distribution",
       y = "Frequency")

```

## PDF's and the Arithmetic Mean of a Distribution

The arithmetic mean value of a distribution is equal to the sum of x multiplied by x's frequency of observation. Since the PDF is approaches zero for any single value, we have to use calculus to represent this:

$$
\mu = \int_{-\infty}^\infty xf(x) dx
$$
It follows then that in a simple random sample of size $n$, each sample observation has a frequency of observation of $\frac{1}{n}$. The sample mean is:  $$\bar{x} = \frac{\sum_{i=1}^{n}x_i}{n}$$.

## Cumulative Distribution Functions

- The cumulative distribution function (CDF) is the probability that some value $X$ is less than or equal to a given value ($x$).

```{r cdf1, echo=FALSE, fig.height=5}
pdf1 <- ggplot(data.frame(x = c(-3.5, 3.5)), aes(x)) +
  stat_function(fun = dnorm) + 
  stat_function(fun = dnorm, 
                xlim = c(-3.5,-2),
                geom = "area", alpha =0.1)  +
    stat_function(fun = dnorm, 
                xlim = c(-3.5,-1),
                geom = "area", alpha =0.1) +
    stat_function(fun = dnorm, 
                xlim = c(-3.5,0),
                geom = "area", alpha =0.1) +
    stat_function(fun = dnorm, 
                xlim = c(-3.5,1),
                geom = "area", alpha =0.1) +
    stat_function(fun = dnorm, 
                xlim = c(-3.5,2),
                geom = "area", alpha =0.1) +
  stat_function(fun = dnorm, 
                xlim = c(-3.5,3.5),
                geom = "area", alpha =0.1) +
                ggtitle("Normal PDF") + ylab("Density") +
  theme_bw(base_family = "sans")



cdf1 <- ggplot(data.frame(x = c(-3.5, 3.5)), aes(x)) +
  stat_function(fun = pnorm) + 
  stat_function(fun = pnorm, 
                xlim = c(-3.5,-2),
                geom = "area", alpha =0.1)  +
    stat_function(fun = pnorm, 
                xlim = c(-3.5,-1),
                geom = "area", alpha =0.1) +
    stat_function(fun = pnorm, 
                xlim = c(-3.5,0),
                geom = "area", alpha =0.1) +
    stat_function(fun = pnorm, 
                xlim = c(-3.5,1),
                geom = "area", alpha =0.1) +
    stat_function(fun = pnorm, 
                xlim = c(-3.5,2),
                geom = "area", alpha =0.1) +
  stat_function(fun = pnorm, 
                xlim = c(-3.5,3.5),
                geom = "area", alpha =0.1) +
                ggtitle("Normal CDF") + ylab(expression("P(x<X)")) +
  theme_bw(base_family = "sans")



xplot <- gridExtra::grid.arrange(grobs = list(pdf1, cdf1), nrow=2, ncol=1)

```




## Survivor Functions
- The survuvor function is 1 - CDF
- The area under the survivor function is the arthimetric mean

```{r sdf1, echo=FALSE, message=FALSE, warning=FALSE}
(sdf1 <- ggplot(data.frame(x = c(-3.5, 3.5)), aes(x)) +
  stat_function(fun = pnorm, args = list(lower.tail=FALSE)) + 
  stat_function(fun = pnorm, args = list(lower.tail=FALSE), 
                xlim = c(-3.5,-2),
                geom = "area", alpha =0.1)  +
    stat_function(fun = pnorm, args = list(lower.tail=FALSE), 
                xlim = c(-3.5,-1),
                geom = "area", alpha =0.1) +
    stat_function(fun = pnorm, args = list(lower.tail=FALSE), 
                xlim = c(-3.5,0),
                geom = "area", alpha =0.1) +
    stat_function(fun = pnorm, args = list(lower.tail=FALSE), 
                xlim = c(-3.5,1),
                geom = "area", alpha =0.1) +
    stat_function(fun = pnorm, args = list(lower.tail=FALSE), 
                xlim = c(-3.5,2),
                geom = "area", alpha =0.1) +
  stat_function(fun = pnorm, args = list(lower.tail=FALSE), 
                xlim = c(-3.5,3.5),
                geom = "area", alpha =0.1) +
                ggtitle("Normal CDF") + ylab(expression("P(x>X)")) +
  theme_bw(base_family = "sans"))

```


## Empirical Distribution Function

- Used for sample data
- The empirical distribution function is like the CDF, but function step $1/n$ at each of the $n$ data points.

```{r ecdf, echo=FALSE}
ggplot(ydata, aes(x, group=sample, col=sample)) + geom_line(stat="ecdf") +
  stat_function(fun=pnorm, col="black", size=1.1) +
  theme_bw(base_family = "sans") + scale_color_brewer("Sample Group", type = "div") + labs(title = "Empirical CDF (4 Samples) \n and Theoretical ECF",
                                                                                           subtitle = "100 samples from N(0,1) distribution") + ylab(expression("P(x<X)"))
```

## Calculating a Mean from a ECDF


# Kaplan-Meier

## ECDF and KM

## Arithmetic Mean and KM


